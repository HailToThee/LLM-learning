{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fc238cb65bd77c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.196854Z",
     "start_time": "2025-07-16T11:32:07.177348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20480\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('the-verdict.txt', 'r') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total characters:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d49f641768b0897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.251115Z",
     "start_time": "2025-07-16T11:32:07.238348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world', '.', ' This is a new text', '.', ' Is there anything good', '?']\n"
     ]
    }
   ],
   "source": [
    "text=\"Hello world. This is a new text. Is there anything good?\"\n",
    "result=re.split(r'([,.?]| \\s)',text)\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e457777dcad887",
   "metadata": {},
   "source": [
    "`item.strip()` 的意思是去除字符串 `item` 两端的空白字符（包括空格、换行、制表符等），返回处理后的新字符串。常用于清理输入或分割后的数据。\n",
    "\n",
    "在`.split`方法中，参数`r''`表示**原始字符串**（raw string）。\n",
    "加上`r`前缀后，字符串中的反斜杠`\\`不会被当作转义字符处理，而是原样保留。这样可以方便地编写正则表达式，比如`\\s`表示匹配空白字符。\n",
    "\n",
    "例如：\n",
    "- `r'\\s'`：匹配空白字符（空格、制表符等）\n",
    "- `'\\n'`：普通字符串，`\\n`会被解析为换行符\n",
    "\n",
    "使用`r''`可以避免转义带来的混淆，尤其在正则表达式中非常常见。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9da2dad84ad8290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.279528Z",
     "start_time": "2025-07-16T11:32:07.269048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:?!;_\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "#对raw_text进行预处理，使用正则表达式将文本分割成单词和标点符号，并去除空白字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e28e9a9e1da006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.336646Z",
     "start_time": "2025-07-16T11:32:07.319516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocal_size= len(all_words)\n",
    "print(vocal_size)\n",
    "\n",
    "vocab = {token:i for i, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    if i < 10:\n",
    "        print(f\"{item[0]}: {item[1]}\")\n",
    "#创建一个词汇表，将所有唯一的单词和标点符号映射到一个唯一的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867e7b080fe9d30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.398809Z",
     "start_time": "2025-07-16T11:32:07.367976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [1, 1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Decoded text: \"\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.stoi = vocab\n",
    "        self.itos = {i:s for s, i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:?!;_\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.stoi[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.itos[i] for i in ids)\n",
    "        text = re.sub(r'\\s+([,.:?!;_\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Encoded IDs:\", ids)\n",
    "print(\"Decoded text:\", tokenizer.decode(ids))\n",
    "\n",
    "# text = \"\"\"hello world. This is a new text.\"\"\"\n",
    "# ids = tokenizer.encode(text)\n",
    "# print(\"Encoded IDs:\", ids)\n",
    "# print(\"Decoded text:\", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5abd8d5f21b7e6",
   "metadata": {},
   "source": [
    "这段代码实现了一个简单的分词器（`SimpleTokenizer`），用于将文本分割成单词和标点，并将它们与唯一索引进行映射。具体说明如下：\n",
    "\n",
    "---\n",
    "\n",
    "### 词汇表创建\n",
    "\n",
    "- 首先，`vocab` 是一个字典，将所有唯一的单词和标点符号映射到唯一的索引（整数）。\n",
    "- 例如：`{'hello': 0, 'world': 1, '.': 2, ...}`\n",
    "\n",
    "### SimpleTokenizer 类\n",
    "\n",
    "- `__init__` 方法：\n",
    "  - `self.stoi` 保存字符串到索引的映射（String to Index）。\n",
    "  - `self.itos` 保存索引到字符串的反向映射（Index to String）。\n",
    "\n",
    "- `encode(text)` 方法：\n",
    "  - 使用正则表达式将文本分割成单词和标点符号。\n",
    "  - 去除空白字符。\n",
    "  - 将每个分割后的 token 映射为对应的索引，返回索引列表。\n",
    "\n",
    "- `decode(ids)` 方法：\n",
    "  - 根据索引列表还原为字符串。\n",
    "  - 用正则表达式处理空格，使标点符号格式更自然。\n",
    "\n",
    "### 使用示例\n",
    "- 创建分词器对象：`tokenizer = SimpleTokenizer(vocab)`\n",
    "- 编码文本为索引：`ids = tokenizer.encode(text)`\n",
    "- 解码索引为文本：`tokenizer.decode(ids)`\n",
    "-\n",
    "但是由于未对不存在的 token 进行处理，可能会导致编码时出现错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c856a00b7f1e4545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:32:07.419702300Z",
     "start_time": "2025-07-15T12:07:48.490006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n",
      "Hello world. This is a new text. <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 1131, 7, 97, 584, 115, 1131, 1131, 7, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|> <|unk|>. This is a <|unk|> <|unk|>. <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表并添加特殊 token\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: i for i, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.stoi = vocab\n",
    "        self.itos = {i: s for s, i in vocab.items()}  # 修正键值顺序\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:?!;_\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = []\n",
    "        for token in preprocessed:\n",
    "            if token in self.stoi:\n",
    "                ids.append(self.stoi[token])\n",
    "            else:\n",
    "                ids.append(self.stoi[\"<|unk|>\"])  # 未知 token\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.itos.get(i, \"<|unk|>\") for i in ids)\n",
    "        text = re.sub(r'\\s+([,.:?!;_\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "text1 = \"Hello world. This is a new text.\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7049d393e965653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:34:21.724660Z",
     "start_time": "2025-07-16T11:34:21.713567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995, 13, 770, 318, 257, 649, 2420, 13, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "Hello world. This is a new text. <|endoftext|> In the sunlit terraces of the palace.\n",
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "##基于BPE的分词器\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text1 = \"Hello world. This is a new text.\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "print(tokenizer.decode(integers))\n",
    "\n",
    "text1 = \"Akwirw ier\"\n",
    "integers = tokenizer.encode(text1,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "print(tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c84c92c3f5e5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:37:30.463223Z",
     "start_time": "2025-07-16T11:37:30.442178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146\n",
      "x:[290, 4920, 2241, 287]\n",
      "y:[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "content_size=4\n",
    "x = enc_sample[:content_size]\n",
    "y = enc_sample[1:content_size+1]\n",
    "print(f\"x:{x}\")\n",
    "print(f\"y:{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0fc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"target_ids\": self.target_ids[idx]\n",
    "        }\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
